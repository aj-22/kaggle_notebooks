{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3ab7cab",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Pipelines are a simple way to keep your data preprocessing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.\n",
    "\n",
    "Many data scientists hack together models without pipelines, but pipelines have some important benefits. Those include:\n",
    "\n",
    "  - <b>Cleaner Code:</b> Accounting for data at each step of preprocessing can get messy. With a pipeline, you won't need to manually keep track of your training and validation data at each step.\n",
    "  - <b>Fewer Bugs:</b> There are fewer opportunities to misapply a step or forget a preprocessing step.\n",
    "  - <b>Easier to Productionize:</b> It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won't go into the many related concerns here, but pipelines can help.\n",
    "  - <b>More Options for Model Validation:</b> You will see an example in the next tutorial, which covers cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f641277",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "As in the previous tutorial, we will work with the Melbourne Housing dataset.\n",
    "\n",
    "We won't focus on the data loading step. Instead, you can imagine you are at a point where you already have the training and validation data in X_train, X_valid, y_train, and y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c158ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"./melb_data.csv\")\n",
    "y = data.Price\n",
    "X = data.drop(['Price'], axis=1)\n",
    "\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X,y,train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3d295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "\n",
    "categorical_columns = [cname for cname in X_train_full.columns \n",
    "                       if X_train_full[cname].nunique() < 10 \n",
    "                       and X_train_full[cname].dtype=='object']\n",
    "\n",
    "numerical_columns = [cname for cname in X_train_full.columns\n",
    "                     if X_train_full[cname].dtype in ['int64','float64']]\n",
    "\n",
    "my_cols = categorical_columns+numerical_columns\n",
    "X_train = X_train_full[my_cols]\n",
    "X_valid = X_valid_full[my_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4af8431d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bedroom2</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Car</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>BuildingArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6586</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>4</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3181.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.85490</td>\n",
       "      <td>144.99480</td>\n",
       "      <td>4380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>t</td>\n",
       "      <td>S</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>13.9</td>\n",
       "      <td>3165.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.92690</td>\n",
       "      <td>145.07870</td>\n",
       "      <td>10969.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6257</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>-37.76310</td>\n",
       "      <td>145.00910</td>\n",
       "      <td>8870.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>u</td>\n",
       "      <td>S</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>9.2</td>\n",
       "      <td>3104.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.79330</td>\n",
       "      <td>145.06720</td>\n",
       "      <td>7809.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11796</th>\n",
       "      <td>h</td>\n",
       "      <td>SA</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>5</td>\n",
       "      <td>20.5</td>\n",
       "      <td>3752.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.62166</td>\n",
       "      <td>145.06747</td>\n",
       "      <td>7969.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Type Method             Regionname  Rooms  Distance  Postcode  Bedroom2  \\\n",
       "6586     h      S  Southern Metropolitan      4       5.1    3181.0       4.0   \n",
       "882      t      S  Southern Metropolitan      3      13.9    3165.0       3.0   \n",
       "6257     h      S  Northern Metropolitan      2       6.5    3071.0       2.0   \n",
       "616      u      S  Southern Metropolitan      3       9.2    3104.0       3.0   \n",
       "11796    h     SA  Northern Metropolitan      5      20.5    3752.0       5.0   \n",
       "\n",
       "       Bathroom  Car  Landsize  BuildingArea  YearBuilt  Lattitude  \\\n",
       "6586        1.0  1.0     230.0           NaN        NaN  -37.85490   \n",
       "882         1.0  1.0     642.0           NaN        NaN  -37.92690   \n",
       "6257        1.0  1.0     372.0         110.0     1950.0  -37.76310   \n",
       "616         2.0  2.0     244.0           NaN        NaN  -37.79330   \n",
       "11796       3.0  4.0     700.0           NaN        NaN  -37.62166   \n",
       "\n",
       "       Longtitude  Propertycount  \n",
       "6586    144.99480         4380.0  \n",
       "882     145.07870        10969.0  \n",
       "6257    145.00910         8870.0  \n",
       "616     145.06720         7809.0  \n",
       "11796   145.06747         7969.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de1ba3",
   "metadata": {},
   "source": [
    "We construct the full pipeline in three steps.\n",
    "## Step 1: Define Preprocessing Steps\n",
    "\n",
    "Similar to how a pipeline bundles together preprocessing and modeling steps, we use the ColumnTransformer class to bundle together different preprocessing steps. The code below:\n",
    "\n",
    "    imputes missing values in numerical data, and\n",
    "    imputes missing values and applies a one-hot encoding to categorical data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec8c15f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=\n",
    "                                   [('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                                    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num',numerical_transformer, numerical_columns),\n",
    "                                              ('cat',categorical_transformer, categorical_columns)])\n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4bc194",
   "metadata": {},
   "source": [
    "\n",
    "# Step 2: Define the Model\n",
    "\n",
    "Next, we define a random forest model with the familiar RandomForestRegressor class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "901b17e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c48ac3",
   "metadata": {},
   "source": [
    "\n",
    "# Step 3: Create and Evaluate the Pipeline\n",
    "\n",
    "Finally, we use the Pipeline class to define a pipeline that bundles the preprocessing and modeling steps. There are a few important things to notice:\n",
    "\n",
    "   - With the pipeline, we preprocess the training data and fit the model in a single line of code. (In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps. This becomes especially messy if we have to deal with both numerical and categorical variables!)\n",
    "    \n",
    "   - With the pipeline, we supply the unprocessed features in X_valid to the predict() command, and the pipeline automatically preprocesses the features before generating predictions. (However, without a pipeline, we have to remember to preprocess the validation data before making predictions.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d8cf85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE= 164956.13011957362\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "my_pipeline = Pipeline(steps = [('preprocessor',preprocessor),\n",
    "                                ('model',model)])\n",
    "\n",
    "my_pipeline.fit(X_train,y_train)\n",
    "\n",
    "preds=my_pipeline.predict(X_valid)\n",
    "\n",
    "score=mean_absolute_error(y_valid, preds)\n",
    "\n",
    "print('MAE=',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a544be71",
   "metadata": {},
   "source": [
    "\n",
    "Conclusion\n",
    "\n",
    "Pipelines are valuable for cleaning up machine learning code and avoiding errors, and are especially useful for workflows with sophisticated data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5069c811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
